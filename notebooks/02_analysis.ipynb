{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask-Dino Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/MaskDINO\n",
      "../datasets\n",
      "../datasets\n",
      "../datasets/coco/train2014\n",
      "../datasets/coco/val2014\n",
      "../datasets/coco/val2014\n",
      "../datasets/coco/val2014\n",
      "../datasets/coco/train2017\n",
      "../datasets/coco/val2017\n",
      "../datasets/coco/test2017\n",
      "../datasets/coco/test2017\n",
      "../datasets/coco/val2017\n",
      "../datasets/coco/train2014\n",
      "../datasets/coco/val2014\n",
      "../datasets/coco/val2014\n",
      "../datasets/coco/val2014\n",
      "../datasets/coco/train2017\n",
      "../datasets/coco/val2017\n",
      "../datasets/coco/val2017\n",
      "../datasets/ADEChallengeData2016/images/training\n",
      "../datasets/ADEChallengeData2016/images/validation\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# fmt: off\n",
    "import sys\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.insert(1, home_dir)\n",
    "print(home_dir)\n",
    "os.environ[\"DETECTRON2_DATASETS\"] = \"../datasets\"\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as mplc\n",
    "import matplotlib.figure as mplfigure\n",
    "import colorsys\n",
    "from pprint import pprint\n",
    "\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data.detection_utils import read_image\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "from detectron2.utils.visualizer import VisImage, _create_text_labels, GenericMask\n",
    "from detectron2.structures import ImageList, BitMasks, Boxes, BoxMode, Keypoints, PolygonMasks, RotatedBoxes\n",
    "from detectron2.utils.colormap import random_color\n",
    "\n",
    "from detectron2.data import (\n",
    "    MetadataCatalog,\n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    ")\n",
    "\n",
    "from maskdino import add_maskdino_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cfg(args):\n",
    "    # load config from file and command-line arguments\n",
    "    cfg = get_cfg()\n",
    "    add_deeplab_config(cfg)\n",
    "    add_maskdino_config(cfg)\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"maskdino demo for builtin configs\")\n",
    "    parser.add_argument(\n",
    "        \"--config-file\",\n",
    "        default=\"../configs/coco/instance-segmentation/maskdino_R50_bs16_50ep_3s.yaml\",\n",
    "        metavar=\"FILE\",\n",
    "        help=\"path to config file\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input\",\n",
    "        nargs=\"+\",\n",
    "        help=\"A list of space separated input images; \"\n",
    "        \"or a single glob pattern such as 'directory/*.jpg'\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--opts\",\n",
    "        help=\"Modify config options using the command-line 'KEY VALUE' pairs\",\n",
    "        default=[],\n",
    "        nargs=argparse.REMAINDER,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        help=\"A file or directory to save output visualizations. \"\n",
    "        \"If not given, will show output in an OpenCV window.\",\n",
    "    )\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method(\"spawn\", force=True)\n",
    "args = get_parser().parse_args('')\n",
    "\n",
    "args.input = [\"../images/fruit.jpg\"]\n",
    "args.opts = ['MODEL.WEIGHTS', '../ckpts/maskdino_r50_50ep_300q_hid1024_3sd1_instance_maskenhanced_mask46.1ap_box51.5ap.pth']\n",
    "args.output = home_dir + \"/outputs\"\n",
    "cfg = setup_cfg(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/djjin/Mygit/MaskDINO'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = build_detection_train_loader(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<detectron2.data.common.AspectRatioGroupedDataset at 0x7f93f804a6d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion.weight_dict  {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_interm': 4.0, 'loss_mask_interm': 5.0, 'loss_dice_interm': 5.0, 'loss_bbox_interm': 5.0, 'loss_giou_interm': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_interm_dn': 4.0, 'loss_mask_interm_dn': 5.0, 'loss_dice_interm_dn': 5.0, 'loss_bbox_interm_dn': 5.0, 'loss_giou_interm_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_interm_0': 4.0, 'loss_mask_interm_0': 5.0, 'loss_dice_interm_0': 5.0, 'loss_bbox_interm_0': 5.0, 'loss_giou_interm_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_interm_dn_0': 4.0, 'loss_mask_interm_dn_0': 5.0, 'loss_dice_interm_dn_0': 5.0, 'loss_bbox_interm_dn_0': 5.0, 'loss_giou_interm_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_interm_1': 4.0, 'loss_mask_interm_1': 5.0, 'loss_dice_interm_1': 5.0, 'loss_bbox_interm_1': 5.0, 'loss_giou_interm_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_interm_dn_1': 4.0, 'loss_mask_interm_dn_1': 5.0, 'loss_dice_interm_dn_1': 5.0, 'loss_bbox_interm_dn_1': 5.0, 'loss_giou_interm_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_interm_2': 4.0, 'loss_mask_interm_2': 5.0, 'loss_dice_interm_2': 5.0, 'loss_bbox_interm_2': 5.0, 'loss_giou_interm_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_interm_dn_2': 4.0, 'loss_mask_interm_dn_2': 5.0, 'loss_dice_interm_dn_2': 5.0, 'loss_bbox_interm_dn_2': 5.0, 'loss_giou_interm_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_interm_3': 4.0, 'loss_mask_interm_3': 5.0, 'loss_dice_interm_3': 5.0, 'loss_bbox_interm_3': 5.0, 'loss_giou_interm_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_interm_dn_3': 4.0, 'loss_mask_interm_dn_3': 5.0, 'loss_dice_interm_dn_3': 5.0, 'loss_bbox_interm_dn_3': 5.0, 'loss_giou_interm_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_interm_4': 4.0, 'loss_mask_interm_4': 5.0, 'loss_dice_interm_4': 5.0, 'loss_bbox_interm_4': 5.0, 'loss_giou_interm_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_interm_dn_4': 4.0, 'loss_mask_interm_dn_4': 5.0, 'loss_dice_interm_dn_4': 5.0, 'loss_bbox_interm_dn_4': 5.0, 'loss_giou_interm_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_interm_5': 4.0, 'loss_mask_interm_5': 5.0, 'loss_dice_interm_5': 5.0, 'loss_bbox_interm_5': 5.0, 'loss_giou_interm_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_interm_dn_5': 4.0, 'loss_mask_interm_dn_5': 5.0, 'loss_dice_interm_dn_5': 5.0, 'loss_bbox_interm_dn_5': 5.0, 'loss_giou_interm_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_interm_6': 4.0, 'loss_mask_interm_6': 5.0, 'loss_dice_interm_6': 5.0, 'loss_bbox_interm_6': 5.0, 'loss_giou_interm_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_interm_dn_6': 4.0, 'loss_mask_interm_dn_6': 5.0, 'loss_dice_interm_dn_6': 5.0, 'loss_bbox_interm_dn_6': 5.0, 'loss_giou_interm_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_interm_7': 4.0, 'loss_mask_interm_7': 5.0, 'loss_dice_interm_7': 5.0, 'loss_bbox_interm_7': 5.0, 'loss_giou_interm_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_interm_dn_7': 4.0, 'loss_mask_interm_dn_7': 5.0, 'loss_dice_interm_dn_7': 5.0, 'loss_bbox_interm_dn_7': 5.0, 'loss_giou_interm_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_interm_8': 4.0, 'loss_mask_interm_8': 5.0, 'loss_dice_interm_8': 5.0, 'loss_bbox_interm_8': 5.0, 'loss_giou_interm_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0, 'loss_ce_interm_dn_8': 4.0, 'loss_mask_interm_dn_8': 5.0, 'loss_dice_interm_dn_8': 5.0, 'loss_bbox_interm_dn_8': 5.0, 'loss_giou_interm_dn_8': 2.0}\n"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg)\n",
    "model.train()\n",
    "\n",
    "if len(cfg.DATASETS.TEST):\n",
    "    metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "\n",
    "checkpointer = DetectionCheckpointer(model)\n",
    "checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "aug = T.ResizeShortestEdge([cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST)\n",
    "input_format = cfg.INPUT.FORMAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone (ResNet)\n",
    "- input shape : [B, 3, H, W]\n",
    "- output shape\n",
    "    - level 1 shape : [B, 256, H/4, W/4]\n",
    "    - level 2 shape : [B, 512, H/8, W/8]\n",
    "    - level 3 shape : [B, 1024, H/16, W/16]\n",
    "    - level 4 shape : [B, 2048, H/32, W/32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Input Shape|\n",
      "  torch.Size([1, 3, 800, 1088])\n",
      "|Backbone Output|\n",
      "  level:0, torch.Size([1, 256, 200, 272])\n",
      "  level:1, torch.Size([1, 512, 100, 136])\n",
      "  level:2, torch.Size([1, 1024, 50, 68])\n",
      "  level:3, torch.Size([1, 2048, 25, 34])\n",
      "|Pixel Decoder Output|\n",
      "  mask_features: torch.Size([1, 256, 200, 272])\n",
      "  transformer_encoder_features: torch.Size([1, 256, 25, 34])\n",
      "  multi_scale_features\n",
      "    level:0, torch.Size([1, 256, 25, 34])\n",
      "    level:1, torch.Size([1, 256, 50, 68])\n",
      "    level:2, torch.Size([1, 256, 100, 136])\n",
      "|Transformer Decoder Output|\n"
     ]
    }
   ],
   "source": [
    "img = read_image(args.input[0], format=\"BGR\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    if input_format == \"RGB\":\n",
    "        # whether the model expects BGR inputs or RGB\n",
    "        original_image = img[:, :, ::-1]\n",
    "    height, width = original_image.shape[:2]\n",
    "    image = aug.get_transform(original_image).apply_image(original_image)\n",
    "    image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "    image.to(cfg.MODEL.DEVICE)\n",
    "\n",
    "    batched_inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "    batched_inputs = [batched_inputs]\n",
    "    images = [x[\"image\"].to(model.device) for x in batched_inputs]\n",
    "    images = [(x - model.pixel_mean) / model.pixel_std for x in images]\n",
    "    images = ImageList.from_tensors(images, model.size_divisibility)\n",
    "    \n",
    "    print(\"|Input Shape|\")\n",
    "    print(f\"  {images.tensor.shape}\")\n",
    "    features = model.backbone(images.tensor)\n",
    "\n",
    "    print(\"|Backbone Output|\")\n",
    "    for lvl, f in enumerate(features):\n",
    "        print(f\"  level:{lvl}, {features[f].shape}\")\n",
    "\n",
    "    print(\"|Pixel Decoder Output|\")\n",
    "    mask_features, transformer_encoder_features, multi_scale_features = model.sem_seg_head.pixel_decoder.forward_features(features, None)\n",
    "    print(f\"  mask_features: {mask_features.shape}\")\n",
    "    print(f\"  transformer_encoder_features: {transformer_encoder_features.shape}\")\n",
    "    print(f\"  multi_scale_features\")\n",
    "    for lvl, f in enumerate(multi_scale_features):\n",
    "        print(f\"    level:{lvl}, {f.shape}\")\n",
    "\n",
    "    print(\"|Transformer Decoder Output|\")\n",
    "    # predictions = model.sem_seg_head.predictor(multi_scale_features, mask_features, None, targets=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(477, 634, 3)\n",
      "torch.Size([3, 800, 1063]) 477 634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/djjin/Mygit/MaskDINO/notebooks/02_analysis.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/djjin/Mygit/MaskDINO/notebooks/02_analysis.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m: image, \u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m: height, \u001b[39m\"\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m\"\u001b[39m: width}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/djjin/Mygit/MaskDINO/notebooks/02_analysis.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(image\u001b[39m.\u001b[39mshape, height, width)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/djjin/Mygit/MaskDINO/notebooks/02_analysis.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m predictions \u001b[39m=\u001b[39m model([inputs])[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/djjin/Mygit/MaskDINO/notebooks/02_analysis.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m out_filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(args\u001b[39m.\u001b[39moutput, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(path))\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_MaskDino/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/MaskDINO/maskdino/maskdino.py:267\u001b[0m, in \u001b[0;36mMaskDINO.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     targets \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m outputs,mask_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msem_seg_head(features,targets\u001b[39m=\u001b[39;49mtargets)\n\u001b[1;32m    268\u001b[0m \u001b[39m# bipartite matching-based loss\u001b[39;00m\n\u001b[1;32m    269\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(outputs, targets,mask_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_MaskDino/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/MaskDINO/maskdino/modeling/meta_arch/maskdino_head.py:75\u001b[0m, in \u001b[0;36mMaskDINOHead.forward\u001b[0;34m(self, features, mask, targets)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, features, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,targets\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(features, mask,targets\u001b[39m=\u001b[39;49mtargets)\n",
      "File \u001b[0;32m~/Mygit/MaskDINO/maskdino/modeling/meta_arch/maskdino_head.py:80\u001b[0m, in \u001b[0;36mMaskDINOHead.layers\u001b[0;34m(self, features, mask, targets)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlayers\u001b[39m(\u001b[39mself\u001b[39m, features, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,targets\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     78\u001b[0m     mask_features, transformer_encoder_features, multi_scale_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpixel_decoder\u001b[39m.\u001b[39mforward_features(features, mask)\n\u001b[0;32m---> 80\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(multi_scale_features, mask_features, mask, targets\u001b[39m=\u001b[39;49mtargets)\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_MaskDino/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/MaskDINO/maskdino/modeling/transformer_decoder/maskdino_decoder.py:446\u001b[0m, in \u001b[0;36mMaskDINODecoder.forward\u001b[0;34m(self, x, mask_features, masks, targets)\u001b[0m\n\u001b[1;32m    444\u001b[0m mask_dict \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdn \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mno\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 446\u001b[0m     \u001b[39massert\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     input_query_label, input_query_bbox, tgt_mask, mask_dict \u001b[39m=\u001b[39m \\\n\u001b[1;32m    448\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_dn(targets, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, x[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    449\u001b[0m     \u001b[39mif\u001b[39;00m mask_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for path in tqdm.tqdm(args.input):\n",
    "    img = read_image(path, format=\"BGR\")\n",
    "\n",
    "    print(img.shape)\n",
    "    with torch.no_grad():\n",
    "        if input_format == \"RGB\":\n",
    "            # whether the model expects BGR inputs or RGB\n",
    "            original_image = img[:, :, ::-1]\n",
    "        height, width = original_image.shape[:2]\n",
    "        image = aug.get_transform(original_image).apply_image(original_image)\n",
    "        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "        image.to(cfg.MODEL.DEVICE)\n",
    "\n",
    "        inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "\n",
    "        print(image.shape, height, width)\n",
    "        predictions = model([inputs])[0]\n",
    "\n",
    "        out_filename = os.path.join(args.output, os.path.basename(path))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img[:, :, ::-1]\n",
    "img = np.asarray(img).clip(0, 255).astype(np.uint8)\n",
    "output = VisImage(img, scale=1.0)\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "instances = predictions[\"instances\"].to(cpu_device)\n",
    "\n",
    "boxes = instances.pred_boxes if instances.has(\"pred_boxes\") else None\n",
    "scores = instances.scores if instances.has(\"scores\") else None\n",
    "classes = instances.pred_classes.tolist() if instances.has(\"pred_classes\") else None\n",
    "labels = _create_text_labels(classes, scores, metadata.get(\"thing_classes\", None))\n",
    "\n",
    "masks = np.asarray(instances.pred_masks)\n",
    "masks = [GenericMask(x, output.height, output.width) for x in masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = None\n",
    "alpha = 0.5\n",
    "default_font_size = max(\n",
    "    np.sqrt(output.height * output.width) // 90, 10 // 1.0\n",
    ")\n",
    "_SMALL_OBJECT_AREA_THRESH = 1000\n",
    "\n",
    "def convert_boxes(boxes):\n",
    "    \"\"\"\n",
    "    Convert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n",
    "    \"\"\"\n",
    "    if isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\n",
    "        return boxes.tensor.detach().numpy()\n",
    "    else:\n",
    "        return np.asarray(boxes)\n",
    "\n",
    "def convert_masks(masks_or_polygons):\n",
    "    \"\"\"\n",
    "    Convert different format of masks or polygons to a tuple of masks and polygons.\n",
    "\n",
    "    Returns:\n",
    "        list[GenericMask]:\n",
    "    \"\"\"\n",
    "\n",
    "    m = masks_or_polygons\n",
    "    if isinstance(m, PolygonMasks):\n",
    "        m = m.polygons\n",
    "    if isinstance(m, BitMasks):\n",
    "        m = m.tensor.numpy()\n",
    "    if isinstance(m, torch.Tensor):\n",
    "        m = m.numpy()\n",
    "    ret = []\n",
    "    for x in m:\n",
    "        if isinstance(x, GenericMask):\n",
    "            ret.append(x)\n",
    "        else:\n",
    "            ret.append(GenericMask(x, output.height, output.width))\n",
    "    return ret\n",
    "\n",
    "def draw_box(output, box_coord, alpha=0.5, edge_color=\"g\", line_style=\"-\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        box_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\n",
    "            are the coordinates of the image's top left corner. x1 and y1 are the\n",
    "            coordinates of the image's bottom right corner.\n",
    "        alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
    "        edge_color: color of the outline of the box. Refer to `matplotlib.colors`\n",
    "            for full list of formats that are accepted.\n",
    "        line_style (string): the string to use to create the outline of the boxes.\n",
    "\n",
    "    Returns:\n",
    "        output (VisImage): image object with box drawn.\n",
    "    \"\"\"\n",
    "    x0, y0, x1, y1 = box_coord\n",
    "    width = x1 - x0\n",
    "    height = y1 - y0\n",
    "\n",
    "    linewidth = max(default_font_size / 4, 1)\n",
    "\n",
    "    output.ax.add_patch(\n",
    "        mpl.patches.Rectangle(\n",
    "            (x0, y0),\n",
    "            width,\n",
    "            height,\n",
    "            fill=False,\n",
    "            edgecolor=edge_color,\n",
    "            linewidth=linewidth * output.scale,\n",
    "            alpha=alpha,\n",
    "            linestyle=line_style,\n",
    "        )\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def draw_polygon(output, segment, color, edge_color=None, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        segment: numpy array of shape Nx2, containing all the points in the polygon.\n",
    "        color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n",
    "            formats that are accepted.\n",
    "        edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n",
    "            full list of formats that are accepted. If not provided, a darker shade\n",
    "            of the polygon color will be used instead.\n",
    "        alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
    "\n",
    "    Returns:\n",
    "        output (VisImage): image object with polygon drawn.\n",
    "    \"\"\"\n",
    "    if edge_color is None:\n",
    "        # make edge color darker than the polygon color\n",
    "        if alpha > 0.8:\n",
    "            edge_color = change_color_brightness(color, brightness_factor=-0.7)\n",
    "        else:\n",
    "            edge_color = color\n",
    "    edge_color = mplc.to_rgb(edge_color) + (1,)\n",
    "\n",
    "    polygon = mpl.patches.Polygon(\n",
    "        segment,\n",
    "        fill=True,\n",
    "        facecolor=mplc.to_rgb(color) + (alpha,),\n",
    "        edgecolor=edge_color,\n",
    "        linewidth=max(default_font_size // 15 * output.scale, 1),\n",
    "    )\n",
    "    output.ax.add_patch(polygon)\n",
    "    return output\n",
    "\n",
    "def change_color_brightness(color, brightness_factor):\n",
    "    \"\"\"\n",
    "    Depending on the brightness_factor, gives a lighter or darker color i.e. a color with\n",
    "    less or more saturation than the original color.\n",
    "\n",
    "    Args:\n",
    "        color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n",
    "            formats that are accepted.\n",
    "        brightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n",
    "            0 will correspond to no change, a factor in [-1.0, 0) range will result in\n",
    "            a darker color and a factor in (0, 1.0] range will result in a lighter color.\n",
    "\n",
    "    Returns:\n",
    "        modified_color (tuple[double]): a tuple containing the RGB values of the\n",
    "            modified color. Each value in the tuple is in the [0.0, 1.0] range.\n",
    "    \"\"\"\n",
    "    assert brightness_factor >= -1.0 and brightness_factor <= 1.0\n",
    "    color = mplc.to_rgb(color)\n",
    "    polygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))\n",
    "    modified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])\n",
    "    modified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness\n",
    "    modified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness\n",
    "    modified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])\n",
    "    return tuple(np.clip(modified_color, 0.0, 1.0))\n",
    "\n",
    "def draw_text(\n",
    "    output,\n",
    "    text,\n",
    "    position,\n",
    "    *,\n",
    "    font_size=None,\n",
    "    color=\"g\",\n",
    "    horizontal_alignment=\"center\",\n",
    "    rotation=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text (str): class label\n",
    "        position (tuple): a tuple of the x and y coordinates to place text on image.\n",
    "        font_size (int, optional): font of the text. If not provided, a font size\n",
    "            proportional to the image width is calculated and used.\n",
    "        color: color of the text. Refer to `matplotlib.colors` for full list\n",
    "            of formats that are accepted.\n",
    "        horizontal_alignment (str): see `matplotlib.text.Text`\n",
    "        rotation: rotation angle in degrees CCW\n",
    "\n",
    "    Returns:\n",
    "        output (VisImage): image object with text drawn.\n",
    "    \"\"\"\n",
    "    if not font_size:\n",
    "        font_size = default_font_size\n",
    "\n",
    "    # since the text background is dark, we don't want the text to be dark\n",
    "    color = np.maximum(list(mplc.to_rgb(color)), 0.2)\n",
    "    color[np.argmax(color)] = max(0.8, np.max(color))\n",
    "\n",
    "    x, y = position\n",
    "    output.ax.text(\n",
    "        x,\n",
    "        y,\n",
    "        text,\n",
    "        size=font_size * output.scale,\n",
    "        family=\"sans-serif\",\n",
    "        bbox={\"facecolor\": \"black\", \"alpha\": 0.8, \"pad\": 0.7, \"edgecolor\": \"none\"},\n",
    "        verticalalignment=\"top\",\n",
    "        horizontalalignment=horizontal_alignment,\n",
    "        color=color,\n",
    "        zorder=10,\n",
    "        rotation=rotation,\n",
    "    )\n",
    "    return output\n",
    "\n",
    "num_instances = 0\n",
    "if boxes is not None:\n",
    "    boxes = convert_boxes(boxes)\n",
    "    num_instances = len(boxes)\n",
    "\n",
    "if masks is not None:\n",
    "    masks = convert_masks(masks)\n",
    "    if num_instances:\n",
    "        assert len(masks) == num_instances\n",
    "    else:\n",
    "        num_instances = len(masks)\n",
    "\n",
    "if labels is not None:\n",
    "    assert len(labels) == num_instances\n",
    "assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = None\n",
    "if boxes is not None:\n",
    "    areas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\n",
    "elif masks is not None:\n",
    "    areas = np.asarray([x.area() for x in masks])\n",
    "\n",
    "if areas is not None:\n",
    "    sorted_idxs = np.argsort(-areas).tolist()\n",
    "    # Re-order overlapped instances in descending order.\n",
    "    boxes = boxes[sorted_idxs] if boxes is not None else None\n",
    "    labels = [labels[k] for k in sorted_idxs] if labels is not None else None\n",
    "    masks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\n",
    "    assigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\n",
    "\n",
    "for i in range(num_instances):\n",
    "    color = assigned_colors[i]\n",
    "    if boxes is not None:\n",
    "        output = draw_box(output, boxes[i], edge_color=color)\n",
    "\n",
    "    if masks is not None:\n",
    "        for segment in masks[i].polygons:\n",
    "            output = draw_polygon(output, segment.reshape(-1, 2), color, alpha=alpha)\n",
    "\n",
    "    if labels is not None:\n",
    "        # first get a box\n",
    "        if boxes is not None:\n",
    "            x0, y0, x1, y1 = boxes[i]\n",
    "            text_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\n",
    "            horiz_align = \"left\"\n",
    "        elif masks is not None:\n",
    "            # skip small mask without polygon\n",
    "            if len(masks[i].polygons) == 0:\n",
    "                continue\n",
    "\n",
    "            x0, y0, x1, y1 = masks[i].bbox()\n",
    "\n",
    "            # draw text in the center (defined by median) when box is not drawn\n",
    "            # median is less sensitive to outliers.\n",
    "            text_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\n",
    "            horiz_align = \"center\"\n",
    "        else:\n",
    "            continue  # drawing the box confidence for keypoints isn't very useful.\n",
    "        # for small objects, draw text at the side to avoid occlusion\n",
    "        instance_area = (y1 - y0) * (x1 - x0)\n",
    "        if (\n",
    "            instance_area < _SMALL_OBJECT_AREA_THRESH * output.scale\n",
    "            or y1 - y0 < 40 * output.scale\n",
    "        ):\n",
    "            if y1 >= output.height - 5:\n",
    "                text_pos = (x1, y0)\n",
    "            else:\n",
    "                text_pos = (x0, y1)\n",
    "\n",
    "        height_ratio = (y1 - y0) / np.sqrt(output.height * output.width)\n",
    "        lighter_color = change_color_brightness(color, brightness_factor=0.7)\n",
    "        font_size = (\n",
    "            np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n",
    "            * 0.5\n",
    "            * default_font_size\n",
    "        )\n",
    "        vis_output = draw_text(\n",
    "            output,\n",
    "            labels[i],\n",
    "            text_pos,\n",
    "            color=lighter_color,\n",
    "            horizontal_alignment=horiz_align,\n",
    "            font_size=font_size,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_MaskDino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
