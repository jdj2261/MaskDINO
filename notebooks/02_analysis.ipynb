{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask-Dino Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/MaskDINO\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# fmt: off\n",
    "import sys\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.insert(1, home_dir)\n",
    "print(home_dir)\n",
    "os.environ[\"DETECTRON2_DATASETS\"] = \"../datasets\"\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as mplc\n",
    "import matplotlib.figure as mplfigure\n",
    "import colorsys\n",
    "from pprint import pprint\n",
    "\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data.detection_utils import read_image\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.data import detection_utils as utils\n",
    "\n",
    "from detectron2.utils.visualizer import VisImage, _create_text_labels, GenericMask\n",
    "from detectron2.structures import ImageList, BitMasks, Boxes, BoxMode, Keypoints, PolygonMasks, RotatedBoxes\n",
    "from detectron2.utils.colormap import random_color\n",
    "\n",
    "from detectron2.data import (\n",
    "    MetadataCatalog,\n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    ")\n",
    "\n",
    "from maskdino.utils import box_ops\n",
    "from maskdino import add_maskdino_config\n",
    "from maskdino import COCOInstanceNewBaselineDatasetMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cfg(args):\n",
    "    # load config from file and command-line arguments\n",
    "    cfg = get_cfg()\n",
    "    add_deeplab_config(cfg)\n",
    "    add_maskdino_config(cfg)\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"maskdino demo for builtin configs\")\n",
    "    parser.add_argument(\n",
    "        \"--config-file\",\n",
    "        default=\"../configs/coco/instance-segmentation/maskdino_R50_bs16_50ep_3s.yaml\",\n",
    "        metavar=\"FILE\",\n",
    "        help=\"path to config file\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input\",\n",
    "        nargs=\"+\",\n",
    "        help=\"A list of space separated input images; \"\n",
    "        \"or a single glob pattern such as 'directory/*.jpg'\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--opts\",\n",
    "        help=\"Modify config options using the command-line 'KEY VALUE' pairs\",\n",
    "        default=[],\n",
    "        nargs=argparse.REMAINDER,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        help=\"A file or directory to save output visualizations. \"\n",
    "        \"If not given, will show output in an OpenCV window.\",\n",
    "    )\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method(\"spawn\", force=True)\n",
    "args = get_parser().parse_args('')\n",
    "\n",
    "args.input = [\"../images/fruit.jpg\"]\n",
    "args.opts = ['MODEL.WEIGHTS', '../ckpts/maskdino_r50_50ep_300q_hid1024_3sd1_instance_maskenhanced_mask46.1ap_box51.5ap.pth']\n",
    "args.output = home_dir + \"/outputs\"\n",
    "cfg = setup_cfg(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CUDNN_BENCHMARK': False,\n",
      " 'DATALOADER': {'ASPECT_RATIO_GROUPING': True,\n",
      "                'FILTER_EMPTY_ANNOTATIONS': True,\n",
      "                'NUM_WORKERS': 4,\n",
      "                'REPEAT_THRESHOLD': 0.0,\n",
      "                'SAMPLER_TRAIN': 'TrainingSampler'},\n",
      " 'DATASETS': {'PRECOMPUTED_PROPOSAL_TOPK_TEST': 1000,\n",
      "              'PRECOMPUTED_PROPOSAL_TOPK_TRAIN': 2000,\n",
      "              'PROPOSAL_FILES_TEST': (),\n",
      "              'PROPOSAL_FILES_TRAIN': (),\n",
      "              'TEST': ('coco_2017_val',),\n",
      "              'TRAIN': ('coco_2017_train',)},\n",
      " 'Default_loading': True,\n",
      " 'GLOBAL': CfgNode({'HACK': 1.0}),\n",
      " 'INPUT': {'COLOR_AUG_SSD': False,\n",
      "           'CROP': {'ENABLED': False,\n",
      "                    'SINGLE_CATEGORY_MAX_AREA': 1.0,\n",
      "                    'SIZE': [0.9, 0.9],\n",
      "                    'TYPE': 'relative_range'},\n",
      "           'DATASET_MAPPER_NAME': 'coco_instance_lsj',\n",
      "           'FORMAT': 'RGB',\n",
      "           'IMAGE_SIZE': 1024,\n",
      "           'MASK_FORMAT': 'polygon',\n",
      "           'MAX_SCALE': 2.0,\n",
      "           'MAX_SIZE_TEST': 1333,\n",
      "           'MAX_SIZE_TRAIN': 1333,\n",
      "           'MIN_SCALE': 0.1,\n",
      "           'MIN_SIZE_TEST': 800,\n",
      "           'MIN_SIZE_TRAIN': (800,),\n",
      "           'MIN_SIZE_TRAIN_SAMPLING': 'choice',\n",
      "           'RANDOM_FLIP': 'horizontal',\n",
      "           'SIZE_DIVISIBILITY': -1},\n",
      " 'MODEL': {'ANCHOR_GENERATOR': {'ANGLES': [[-90, 0, 90]],\n",
      "                                'ASPECT_RATIOS': [[0.5, 1.0, 2.0]],\n",
      "                                'NAME': 'DefaultAnchorGenerator',\n",
      "                                'OFFSET': 0.0,\n",
      "                                'SIZES': [[32, 64, 128, 256, 512]]},\n",
      "           'BACKBONE': {'FREEZE_AT': 0,\n",
      "                        'NAME': 'build_resnet_backbone'},\n",
      "           'DEVICE': 'cuda',\n",
      "           'FPN': {'FUSE_TYPE': 'sum',\n",
      "                   'IN_FEATURES': [],\n",
      "                   'NORM': '',\n",
      "                   'OUT_CHANNELS': 256},\n",
      "           'KEYPOINT_ON': False,\n",
      "           'LOAD_PROPOSALS': False,\n",
      "           'MASK_ON': False,\n",
      "           'META_ARCHITECTURE': 'MaskDINO',\n",
      "           'MaskDINO': {'BOX_LOSS': True,\n",
      "                        'BOX_WEIGHT': 5.0,\n",
      "                        'CLASS_WEIGHT': 4.0,\n",
      "                        'COST_BOX_WEIGHT': 5.0,\n",
      "                        'COST_CLASS_WEIGHT': 4.0,\n",
      "                        'COST_DICE_WEIGHT': 5.0,\n",
      "                        'COST_GIOU_WEIGHT': 2.0,\n",
      "                        'COST_MASK_WEIGHT': 5.0,\n",
      "                        'DEC_LAYERS': 9,\n",
      "                        'DEEP_SUPERVISION': True,\n",
      "                        'DICE_WEIGHT': 5.0,\n",
      "                        'DIM_FEEDFORWARD': 2048,\n",
      "                        'DN': 'seg',\n",
      "                        'DN_NOISE_SCALE': 0.4,\n",
      "                        'DN_NUM': 100,\n",
      "                        'DROPOUT': 0.0,\n",
      "                        'ENC_LAYERS': 0,\n",
      "                        'ENFORCE_INPUT_PROJ': False,\n",
      "                        'EVAL_FLAG': 1,\n",
      "                        'GIOU_WEIGHT': 2.0,\n",
      "                        'HIDDEN_DIM': 256,\n",
      "                        'IMPORTANCE_SAMPLE_RATIO': 0.75,\n",
      "                        'INITIALIZE_BOX_TYPE': 'bitmask',\n",
      "                        'INITIAL_PRED': True,\n",
      "                        'LEARN_TGT': False,\n",
      "                        'MASK_WEIGHT': 5.0,\n",
      "                        'NHEADS': 8,\n",
      "                        'NO_OBJECT_WEIGHT': 0.1,\n",
      "                        'NUM_OBJECT_QUERIES': 300,\n",
      "                        'OVERSAMPLE_RATIO': 3.0,\n",
      "                        'PANO_BOX_LOSS': False,\n",
      "                        'PRED_CONV': False,\n",
      "                        'PRE_NORM': False,\n",
      "                        'SEMANTIC_CE_LOSS': False,\n",
      "                        'SIZE_DIVISIBILITY': 32,\n",
      "                        'TEST': {'INSTANCE_ON': True,\n",
      "                                 'OBJECT_MASK_THRESHOLD': 0.25,\n",
      "                                 'OVERLAP_THRESHOLD': 0.8,\n",
      "                                 'PANOPTIC_ON': False,\n",
      "                                 'PANO_TEMPERATURE': 0.06,\n",
      "                                 'PANO_TRANSFORM_EVAL': True,\n",
      "                                 'SEMANTIC_ON': False,\n",
      "                                 'SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE': False,\n",
      "                                 'TEST_FOUCUS_ON_BOX': False},\n",
      "                        'TRAIN_NUM_POINTS': 12544,\n",
      "                        'TRANSFORMER_DECODER_NAME': 'MaskDINODecoder',\n",
      "                        'TWO_STAGE': True},\n",
      "           'PANOPTIC_FPN': {'COMBINE': {'ENABLED': True,\n",
      "                                        'INSTANCES_CONFIDENCE_THRESH': 0.5,\n",
      "                                        'OVERLAP_THRESH': 0.5,\n",
      "                                        'STUFF_AREA_LIMIT': 4096},\n",
      "                            'INSTANCE_LOSS_WEIGHT': 1.0},\n",
      "           'PIXEL_MEAN': [123.675, 116.28, 103.53],\n",
      "           'PIXEL_STD': [58.395, 57.12, 57.375],\n",
      "           'PROPOSAL_GENERATOR': CfgNode({'NAME': 'RPN', 'MIN_SIZE': 0}),\n",
      "           'RESNETS': {'DEFORM_MODULATED': False,\n",
      "                       'DEFORM_NUM_GROUPS': 1,\n",
      "                       'DEFORM_ON_PER_STAGE': [False, False, False, False],\n",
      "                       'DEPTH': 50,\n",
      "                       'NORM': 'FrozenBN',\n",
      "                       'NUM_GROUPS': 1,\n",
      "                       'OUT_FEATURES': ['res2', 'res3', 'res4', 'res5'],\n",
      "                       'RES2_OUT_CHANNELS': 256,\n",
      "                       'RES4_DILATION': 1,\n",
      "                       'RES5_DILATION': 1,\n",
      "                       'RES5_MULTI_GRID': [1, 1, 1],\n",
      "                       'STEM_OUT_CHANNELS': 64,\n",
      "                       'STEM_TYPE': 'basic',\n",
      "                       'STRIDE_IN_1X1': False,\n",
      "                       'WIDTH_PER_GROUP': 64},\n",
      "           'RETINANET': {'BBOX_REG_LOSS_TYPE': 'smooth_l1',\n",
      "                         'BBOX_REG_WEIGHTS': (1.0, 1.0, 1.0, 1.0),\n",
      "                         'FOCAL_LOSS_ALPHA': 0.25,\n",
      "                         'FOCAL_LOSS_GAMMA': 2.0,\n",
      "                         'IN_FEATURES': ['p3', 'p4', 'p5', 'p6', 'p7'],\n",
      "                         'IOU_LABELS': [0, -1, 1],\n",
      "                         'IOU_THRESHOLDS': [0.4, 0.5],\n",
      "                         'NMS_THRESH_TEST': 0.5,\n",
      "                         'NORM': '',\n",
      "                         'NUM_CLASSES': 80,\n",
      "                         'NUM_CONVS': 4,\n",
      "                         'PRIOR_PROB': 0.01,\n",
      "                         'SCORE_THRESH_TEST': 0.05,\n",
      "                         'SMOOTH_L1_LOSS_BETA': 0.1,\n",
      "                         'TOPK_CANDIDATES_TEST': 1000},\n",
      "           'ROI_BOX_CASCADE_HEAD': {'BBOX_REG_WEIGHTS': ((10.0, 10.0, 5.0, 5.0),\n",
      "                                                         (20.0,\n",
      "                                                          20.0,\n",
      "                                                          10.0,\n",
      "                                                          10.0),\n",
      "                                                         (30.0,\n",
      "                                                          30.0,\n",
      "                                                          15.0,\n",
      "                                                          15.0)),\n",
      "                                    'IOUS': (0.5, 0.6, 0.7)},\n",
      "           'ROI_BOX_HEAD': {'BBOX_REG_LOSS_TYPE': 'smooth_l1',\n",
      "                            'BBOX_REG_LOSS_WEIGHT': 1.0,\n",
      "                            'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),\n",
      "                            'CLS_AGNOSTIC_BBOX_REG': False,\n",
      "                            'CONV_DIM': 256,\n",
      "                            'FC_DIM': 1024,\n",
      "                            'FED_LOSS_FREQ_WEIGHT_POWER': 0.5,\n",
      "                            'FED_LOSS_NUM_CLASSES': 50,\n",
      "                            'NAME': '',\n",
      "                            'NORM': '',\n",
      "                            'NUM_CONV': 0,\n",
      "                            'NUM_FC': 0,\n",
      "                            'POOLER_RESOLUTION': 14,\n",
      "                            'POOLER_SAMPLING_RATIO': 0,\n",
      "                            'POOLER_TYPE': 'ROIAlignV2',\n",
      "                            'SMOOTH_L1_BETA': 0.0,\n",
      "                            'TRAIN_ON_PRED_BOXES': False,\n",
      "                            'USE_FED_LOSS': False,\n",
      "                            'USE_SIGMOID_CE': False},\n",
      "           'ROI_HEADS': {'BATCH_SIZE_PER_IMAGE': 512,\n",
      "                         'IN_FEATURES': ['res4'],\n",
      "                         'IOU_LABELS': [0, 1],\n",
      "                         'IOU_THRESHOLDS': [0.5],\n",
      "                         'NAME': 'Res5ROIHeads',\n",
      "                         'NMS_THRESH_TEST': 0.5,\n",
      "                         'NUM_CLASSES': 80,\n",
      "                         'POSITIVE_FRACTION': 0.25,\n",
      "                         'PROPOSAL_APPEND_GT': True,\n",
      "                         'SCORE_THRESH_TEST': 0.05},\n",
      "           'ROI_KEYPOINT_HEAD': {'CONV_DIMS': (512,\n",
      "                                               512,\n",
      "                                               512,\n",
      "                                               512,\n",
      "                                               512,\n",
      "                                               512,\n",
      "                                               512,\n",
      "                                               512),\n",
      "                                 'LOSS_WEIGHT': 1.0,\n",
      "                                 'MIN_KEYPOINTS_PER_IMAGE': 1,\n",
      "                                 'NAME': 'KRCNNConvDeconvUpsampleHead',\n",
      "                                 'NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS': True,\n",
      "                                 'NUM_KEYPOINTS': 17,\n",
      "                                 'POOLER_RESOLUTION': 14,\n",
      "                                 'POOLER_SAMPLING_RATIO': 0,\n",
      "                                 'POOLER_TYPE': 'ROIAlignV2'},\n",
      "           'ROI_MASK_HEAD': {'CLS_AGNOSTIC_MASK': False,\n",
      "                             'CONV_DIM': 256,\n",
      "                             'NAME': 'MaskRCNNConvUpsampleHead',\n",
      "                             'NORM': '',\n",
      "                             'NUM_CONV': 0,\n",
      "                             'POOLER_RESOLUTION': 14,\n",
      "                             'POOLER_SAMPLING_RATIO': 0,\n",
      "                             'POOLER_TYPE': 'ROIAlignV2'},\n",
      "           'RPN': {'BATCH_SIZE_PER_IMAGE': 256,\n",
      "                   'BBOX_REG_LOSS_TYPE': 'smooth_l1',\n",
      "                   'BBOX_REG_LOSS_WEIGHT': 1.0,\n",
      "                   'BBOX_REG_WEIGHTS': (1.0, 1.0, 1.0, 1.0),\n",
      "                   'BOUNDARY_THRESH': -1,\n",
      "                   'CONV_DIMS': [-1],\n",
      "                   'HEAD_NAME': 'StandardRPNHead',\n",
      "                   'IN_FEATURES': ['res4'],\n",
      "                   'IOU_LABELS': [0, -1, 1],\n",
      "                   'IOU_THRESHOLDS': [0.3, 0.7],\n",
      "                   'LOSS_WEIGHT': 1.0,\n",
      "                   'NMS_THRESH': 0.7,\n",
      "                   'POSITIVE_FRACTION': 0.5,\n",
      "                   'POST_NMS_TOPK_TEST': 1000,\n",
      "                   'POST_NMS_TOPK_TRAIN': 2000,\n",
      "                   'PRE_NMS_TOPK_TEST': 6000,\n",
      "                   'PRE_NMS_TOPK_TRAIN': 12000,\n",
      "                   'SMOOTH_L1_BETA': 0.0},\n",
      "           'SEM_SEG_HEAD': {'ASPP_CHANNELS': 256,\n",
      "                            'ASPP_DILATIONS': [6, 12, 18],\n",
      "                            'ASPP_DROPOUT': 0.1,\n",
      "                            'COMMON_STRIDE': 4,\n",
      "                            'CONVS_DIM': 256,\n",
      "                            'DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES': ['res3',\n",
      "                                                                           'res4',\n",
      "                                                                           'res5'],\n",
      "                            'DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS': 8,\n",
      "                            'DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS': 4,\n",
      "                            'DIM_FEEDFORWARD': 1024,\n",
      "                            'FEATURE_ORDER': 'high2low',\n",
      "                            'IGNORE_VALUE': 255,\n",
      "                            'IN_FEATURES': ['res2', 'res3', 'res4', 'res5'],\n",
      "                            'LOSS_TYPE': 'hard_pixel_mining',\n",
      "                            'LOSS_WEIGHT': 1.0,\n",
      "                            'MASK_DIM': 256,\n",
      "                            'NAME': 'MaskDINOHead',\n",
      "                            'NORM': 'GN',\n",
      "                            'NUM_CLASSES': 80,\n",
      "                            'NUM_FEATURE_LEVELS': 3,\n",
      "                            'PIXEL_DECODER_NAME': 'MaskDINOEncoder',\n",
      "                            'PROJECT_CHANNELS': [48],\n",
      "                            'PROJECT_FEATURES': ['res2'],\n",
      "                            'TOTAL_NUM_FEATURE_LEVELS': 3,\n",
      "                            'TRANSFORMER_ENC_LAYERS': 6,\n",
      "                            'USE_DEPTHWISE_SEPARABLE_CONV': False},\n",
      "           'SWIN': {'APE': False,\n",
      "                    'ATTN_DROP_RATE': 0.0,\n",
      "                    'DEPTHS': [2, 2, 6, 2],\n",
      "                    'DROP_PATH_RATE': 0.3,\n",
      "                    'DROP_RATE': 0.0,\n",
      "                    'EMBED_DIM': 96,\n",
      "                    'MLP_RATIO': 4.0,\n",
      "                    'NUM_HEADS': [3, 6, 12, 24],\n",
      "                    'OUT_FEATURES': ['res2', 'res3', 'res4', 'res5'],\n",
      "                    'PATCH_NORM': True,\n",
      "                    'PATCH_SIZE': 4,\n",
      "                    'PRETRAIN_IMG_SIZE': 224,\n",
      "                    'QKV_BIAS': True,\n",
      "                    'QK_SCALE': None,\n",
      "                    'USE_CHECKPOINT': False,\n",
      "                    'WINDOW_SIZE': 7},\n",
      "           'WEIGHTS': '../ckpts/maskdino_r50_50ep_300q_hid1024_3sd1_instance_maskenhanced_mask46.1ap_box51.5ap.pth'},\n",
      " 'OUTPUT_DIR': './output',\n",
      " 'SEED': -1,\n",
      " 'SOLVER': {'AMP': CfgNode({'ENABLED': True}),\n",
      "            'BACKBONE_MULTIPLIER': 0.1,\n",
      "            'BASE_LR': 0.0001,\n",
      "            'BASE_LR_END': 0.0,\n",
      "            'BIAS_LR_FACTOR': 1.0,\n",
      "            'CHECKPOINT_PERIOD': 5000,\n",
      "            'CLIP_GRADIENTS': {'CLIP_TYPE': 'full_model',\n",
      "                               'CLIP_VALUE': 0.01,\n",
      "                               'ENABLED': True,\n",
      "                               'NORM_TYPE': 2.0},\n",
      "            'GAMMA': 0.1,\n",
      "            'IMS_PER_BATCH': 2,\n",
      "            'LR_SCHEDULER_NAME': 'WarmupMultiStepLR',\n",
      "            'MAX_ITER': 368750,\n",
      "            'MOMENTUM': 0.9,\n",
      "            'NESTEROV': False,\n",
      "            'NUM_DECAYS': 3,\n",
      "            'OPTIMIZER': 'ADAMW',\n",
      "            'POLY_LR_CONSTANT_ENDING': 0.0,\n",
      "            'POLY_LR_POWER': 0.9,\n",
      "            'REFERENCE_WORLD_SIZE': 0,\n",
      "            'RESCALE_INTERVAL': False,\n",
      "            'STEPS': (327778, 355092),\n",
      "            'WARMUP_FACTOR': 1.0,\n",
      "            'WARMUP_ITERS': 10,\n",
      "            'WARMUP_METHOD': 'linear',\n",
      "            'WEIGHT_DECAY': 0.05,\n",
      "            'WEIGHT_DECAY_BIAS': None,\n",
      "            'WEIGHT_DECAY_EMBED': 0.0,\n",
      "            'WEIGHT_DECAY_NORM': 0.0},\n",
      " 'TEST': {'AUG': {'ENABLED': False,\n",
      "                  'FLIP': True,\n",
      "                  'MAX_SIZE': 4000,\n",
      "                  'MIN_SIZES': (400,\n",
      "                                500,\n",
      "                                600,\n",
      "                                700,\n",
      "                                800,\n",
      "                                900,\n",
      "                                1000,\n",
      "                                1100,\n",
      "                                1200)},\n",
      "          'DETECTIONS_PER_IMAGE': 100,\n",
      "          'EVAL_PERIOD': 5000,\n",
      "          'EXPECTED_RESULTS': [],\n",
      "          'KEYPOINT_OKS_SIGMAS': [],\n",
      "          'PRECISE_BN': CfgNode({'ENABLED': False, 'NUM_ITER': 200})},\n",
      " 'VERSION': 2,\n",
      " 'VIS_PERIOD': 0}\n"
     ]
    }
   ],
   "source": [
    "pprint(cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = COCOInstanceNewBaselineDatasetMapper(cfg, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = build_detection_train_loader(cfg, mapper=mapper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion.weight_dict  {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_interm': 4.0, 'loss_mask_interm': 5.0, 'loss_dice_interm': 5.0, 'loss_bbox_interm': 5.0, 'loss_giou_interm': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_interm_dn': 4.0, 'loss_mask_interm_dn': 5.0, 'loss_dice_interm_dn': 5.0, 'loss_bbox_interm_dn': 5.0, 'loss_giou_interm_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_interm_0': 4.0, 'loss_mask_interm_0': 5.0, 'loss_dice_interm_0': 5.0, 'loss_bbox_interm_0': 5.0, 'loss_giou_interm_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_interm_dn_0': 4.0, 'loss_mask_interm_dn_0': 5.0, 'loss_dice_interm_dn_0': 5.0, 'loss_bbox_interm_dn_0': 5.0, 'loss_giou_interm_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_interm_1': 4.0, 'loss_mask_interm_1': 5.0, 'loss_dice_interm_1': 5.0, 'loss_bbox_interm_1': 5.0, 'loss_giou_interm_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_interm_dn_1': 4.0, 'loss_mask_interm_dn_1': 5.0, 'loss_dice_interm_dn_1': 5.0, 'loss_bbox_interm_dn_1': 5.0, 'loss_giou_interm_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_interm_2': 4.0, 'loss_mask_interm_2': 5.0, 'loss_dice_interm_2': 5.0, 'loss_bbox_interm_2': 5.0, 'loss_giou_interm_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_interm_dn_2': 4.0, 'loss_mask_interm_dn_2': 5.0, 'loss_dice_interm_dn_2': 5.0, 'loss_bbox_interm_dn_2': 5.0, 'loss_giou_interm_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_interm_3': 4.0, 'loss_mask_interm_3': 5.0, 'loss_dice_interm_3': 5.0, 'loss_bbox_interm_3': 5.0, 'loss_giou_interm_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_interm_dn_3': 4.0, 'loss_mask_interm_dn_3': 5.0, 'loss_dice_interm_dn_3': 5.0, 'loss_bbox_interm_dn_3': 5.0, 'loss_giou_interm_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_interm_4': 4.0, 'loss_mask_interm_4': 5.0, 'loss_dice_interm_4': 5.0, 'loss_bbox_interm_4': 5.0, 'loss_giou_interm_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_interm_dn_4': 4.0, 'loss_mask_interm_dn_4': 5.0, 'loss_dice_interm_dn_4': 5.0, 'loss_bbox_interm_dn_4': 5.0, 'loss_giou_interm_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_interm_5': 4.0, 'loss_mask_interm_5': 5.0, 'loss_dice_interm_5': 5.0, 'loss_bbox_interm_5': 5.0, 'loss_giou_interm_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_interm_dn_5': 4.0, 'loss_mask_interm_dn_5': 5.0, 'loss_dice_interm_dn_5': 5.0, 'loss_bbox_interm_dn_5': 5.0, 'loss_giou_interm_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_interm_6': 4.0, 'loss_mask_interm_6': 5.0, 'loss_dice_interm_6': 5.0, 'loss_bbox_interm_6': 5.0, 'loss_giou_interm_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_interm_dn_6': 4.0, 'loss_mask_interm_dn_6': 5.0, 'loss_dice_interm_dn_6': 5.0, 'loss_bbox_interm_dn_6': 5.0, 'loss_giou_interm_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_interm_7': 4.0, 'loss_mask_interm_7': 5.0, 'loss_dice_interm_7': 5.0, 'loss_bbox_interm_7': 5.0, 'loss_giou_interm_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_interm_dn_7': 4.0, 'loss_mask_interm_dn_7': 5.0, 'loss_dice_interm_dn_7': 5.0, 'loss_bbox_interm_dn_7': 5.0, 'loss_giou_interm_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_interm_8': 4.0, 'loss_mask_interm_8': 5.0, 'loss_dice_interm_8': 5.0, 'loss_bbox_interm_8': 5.0, 'loss_giou_interm_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0, 'loss_ce_interm_dn_8': 4.0, 'loss_mask_interm_dn_8': 5.0, 'loss_dice_interm_dn_8': 5.0, 'loss_bbox_interm_dn_8': 5.0, 'loss_giou_interm_dn_8': 2.0}\n"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg)\n",
    "model.train()\n",
    "\n",
    "if len(cfg.DATASETS.TEST):\n",
    "    metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "\n",
    "checkpointer = DetectionCheckpointer(model)\n",
    "checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "aug = T.ResizeShortestEdge([cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST)\n",
    "input_format = cfg.INPUT.FORMAT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone (ResNet)\n",
    "- input shape : [B, 3, H, W]\n",
    "- output shape\n",
    "    - level 1 shape : [B, 256, H/4, W/4]\n",
    "    - level 2 shape : [B, 512, H/8, W/8]\n",
    "    - level 3 shape : [B, 1024, H/16, W/16]\n",
    "    - level 4 shape : [B, 2048, H/32, W/32]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Decoder\n",
    "- Input : Backbone features\n",
    "- Output\n",
    "  - mask_features shape : [B, 256, H/4, W/4] --> For Mask2Former\n",
    "  - transformer_encoder_features : [B, 2048, H/32, W/32] --> Not used\n",
    "  - multi_scale_features --> For Deformable DETR\n",
    "    - level 2 shape : [B, 512, H/8, W/8]\n",
    "    - level 3 shape : [B, 1024, H/16, W/16]\n",
    "    - level 4 shape : [B, 2048, H/32, W/32]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Decoder\n",
    "- Input\n",
    "  - multi_scale_features\n",
    "  - mask_features\n",
    "  - Mask (None)\n",
    "  - targets\n",
    "- Output\n",
    "  - outputs\n",
    "  - mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_targets(targets, images):\n",
    "    h_pad, w_pad = images.tensor.shape[-2:]\n",
    "    new_targets = []\n",
    "    for targets_per_image in targets:\n",
    "        # pad gt\n",
    "        h, w = targets_per_image.image_size\n",
    "        image_size_xyxy = torch.as_tensor([w, h, w, h], dtype=torch.float, device=model.device)\n",
    "\n",
    "        gt_masks = targets_per_image.gt_masks\n",
    "        padded_masks = torch.zeros((gt_masks.shape[0], h_pad, w_pad), dtype=gt_masks.dtype, device=gt_masks.device)\n",
    "        padded_masks[:, : gt_masks.shape[1], : gt_masks.shape[2]] = gt_masks\n",
    "        new_targets.append(\n",
    "            {\n",
    "                \"labels\": targets_per_image.gt_classes,\n",
    "                \"masks\": padded_masks,\n",
    "                \"boxes\":box_ops.box_xyxy_to_cxcywh(targets_per_image.gt_boxes.tensor)/image_size_xyxy\n",
    "            }\n",
    "        )\n",
    "    return new_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_iter_obj = iter(data_loader)\n",
    "data = next(data_loader_iter_obj)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_dict = model(data)\n",
    "# print(loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Input Shape|\n",
      "  torch.Size([2, 3, 1024, 1024])\n",
      "|Backbone Output|\n",
      "  level:0, torch.Size([2, 256, 256, 256])\n",
      "  level:1, torch.Size([2, 512, 128, 128])\n",
      "  level:2, torch.Size([2, 1024, 64, 64])\n",
      "  level:3, torch.Size([2, 2048, 32, 32])\n",
      "|Pixel Decoder Output|\n",
      "  mask_features: torch.Size([2, 256, 256, 256])\n",
      "  transformer_encoder_features: torch.Size([2, 256, 32, 32])\n",
      "  multi_scale_features\n",
      "    level:0, torch.Size([2, 256, 32, 32])\n",
      "    level:1, torch.Size([2, 256, 64, 64])\n",
      "    level:2, torch.Size([2, 256, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    images = [x[\"image\"].to(model.device) for x in data]\n",
    "    images = [(x - model.pixel_mean) / model.pixel_std for x in images]\n",
    "    images = ImageList.from_tensors(images, model.size_divisibility)\n",
    "    \n",
    "    print(\"|Input Shape|\")\n",
    "    print(f\"  {images.tensor.shape}\")\n",
    "    features = model.backbone(images.tensor)\n",
    "\n",
    "    print(\"|Backbone Output|\")\n",
    "    for lvl, f in enumerate(features):\n",
    "        print(f\"  level:{lvl}, {features[f].shape}\")\n",
    "\n",
    "    gt_instances = [x[\"instances\"].to(model.device) for x in data]\n",
    "    targets = prepare_targets(gt_instances, images)\n",
    "\n",
    "    print(\"|Pixel Decoder Output|\")\n",
    "    mask_features, transformer_encoder_features, multi_scale_features = model.sem_seg_head.pixel_decoder.forward_features(features, None)\n",
    "    print(f\"  mask_features: {mask_features.shape}\")\n",
    "    print(f\"  transformer_encoder_features: {transformer_encoder_features.shape}\")\n",
    "    print(f\"  multi_scale_features\")\n",
    "    for lvl, f in enumerate(multi_scale_features):\n",
    "        print(f\"    level:{lvl}, {f.shape}\")\n",
    "\n",
    "    # print(\"|Transformer Decoder Output|\")\n",
    "    # outputs, mask_dict = model.sem_seg_head.predictor(multi_scale_features, mask_features, None, targets=targets)\n",
    "    # print(outputs.keys(), mask_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(477, 634, 3)\n",
      "torch.Size([3, 800, 1063]) 477 634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for path in tqdm.tqdm(args.input):\n",
    "    img = read_image(path, format=\"BGR\")\n",
    "\n",
    "    print(img.shape)\n",
    "    with torch.no_grad():\n",
    "        if input_format == \"RGB\":\n",
    "            # whether the model expects BGR inputs or RGB\n",
    "            original_image = img[:, :, ::-1]\n",
    "        height, width = original_image.shape[:2]\n",
    "        image = aug.get_transform(original_image).apply_image(original_image)\n",
    "        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "        image.to(cfg.MODEL.DEVICE)\n",
    "\n",
    "        inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "\n",
    "        print(image.shape, height, width)\n",
    "        predictions = model([inputs])[0]\n",
    "\n",
    "        out_filename = os.path.join(args.output, os.path.basename(path))\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img[:, :, ::-1]\n",
    "img = np.asarray(img).clip(0, 255).astype(np.uint8)\n",
    "output = VisImage(img, scale=1.0)\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "instances = predictions[\"instances\"].to(cpu_device)\n",
    "\n",
    "boxes = instances.pred_boxes if instances.has(\"pred_boxes\") else None\n",
    "scores = instances.scores if instances.has(\"scores\") else None\n",
    "classes = instances.pred_classes.tolist() if instances.has(\"pred_classes\") else None\n",
    "labels = _create_text_labels(classes, scores, metadata.get(\"thing_classes\", None))\n",
    "\n",
    "masks = np.asarray(instances.pred_masks)\n",
    "masks = [GenericMask(x, output.height, output.width) for x in masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = None\n",
    "alpha = 0.5\n",
    "default_font_size = max(\n",
    "    np.sqrt(output.height * output.width) // 90, 10 // 1.0\n",
    ")\n",
    "_SMALL_OBJECT_AREA_THRESH = 1000\n",
    "\n",
    "def convert_boxes(boxes):\n",
    "    \"\"\"\n",
    "    Convert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n",
    "    \"\"\"\n",
    "    if isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\n",
    "        return boxes.tensor.detach().numpy()\n",
    "    else:\n",
    "        return np.asarray(boxes)\n",
    "\n",
    "def convert_masks(masks_or_polygons):\n",
    "    \"\"\"\n",
    "    Convert different format of masks or polygons to a tuple of masks and polygons.\n",
    "\n",
    "    Returns:\n",
    "        list[GenericMask]:\n",
    "    \"\"\"\n",
    "\n",
    "    m = masks_or_polygons\n",
    "    if isinstance(m, PolygonMasks):\n",
    "        m = m.polygons\n",
    "    if isinstance(m, BitMasks):\n",
    "        m = m.tensor.numpy()\n",
    "    if isinstance(m, torch.Tensor):\n",
    "        m = m.numpy()\n",
    "    ret = []\n",
    "    for x in m:\n",
    "        if isinstance(x, GenericMask):\n",
    "            ret.append(x)\n",
    "        else:\n",
    "            ret.append(GenericMask(x, output.height, output.width))\n",
    "    return ret\n",
    "\n",
    "def draw_box(output, box_coord, alpha=0.5, edge_color=\"g\", line_style=\"-\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        box_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\n",
    "            are the coordinates of the image's top left corner. x1 and y1 are the\n",
    "            coordinates of the image's bottom right corner.\n",
    "        alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
    "        edge_color: color of the outline of the box. Refer to `matplotlib.colors`\n",
    "            for full list of formats that are accepted.\n",
    "        line_style (string): the string to use to create the outline of the boxes.\n",
    "\n",
    "    Returns:\n",
    "        output (VisImage): image object with box drawn.\n",
    "    \"\"\"\n",
    "    x0, y0, x1, y1 = box_coord\n",
    "    width = x1 - x0\n",
    "    height = y1 - y0\n",
    "\n",
    "    linewidth = max(default_font_size / 4, 1)\n",
    "\n",
    "    output.ax.add_patch(\n",
    "        mpl.patches.Rectangle(\n",
    "            (x0, y0),\n",
    "            width,\n",
    "            height,\n",
    "            fill=False,\n",
    "            edgecolor=edge_color,\n",
    "            linewidth=linewidth * output.scale,\n",
    "            alpha=alpha,\n",
    "            linestyle=line_style,\n",
    "        )\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def draw_polygon(output, segment, color, edge_color=None, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        segment: numpy array of shape Nx2, containing all the points in the polygon.\n",
    "        color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n",
    "            formats that are accepted.\n",
    "        edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n",
    "            full list of formats that are accepted. If not provided, a darker shade\n",
    "            of the polygon color will be used instead.\n",
    "        alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
    "\n",
    "    Returns:\n",
    "        output (VisImage): image object with polygon drawn.\n",
    "    \"\"\"\n",
    "    if edge_color is None:\n",
    "        # make edge color darker than the polygon color\n",
    "        if alpha > 0.8:\n",
    "            edge_color = change_color_brightness(color, brightness_factor=-0.7)\n",
    "        else:\n",
    "            edge_color = color\n",
    "    edge_color = mplc.to_rgb(edge_color) + (1,)\n",
    "\n",
    "    polygon = mpl.patches.Polygon(\n",
    "        segment,\n",
    "        fill=True,\n",
    "        facecolor=mplc.to_rgb(color) + (alpha,),\n",
    "        edgecolor=edge_color,\n",
    "        linewidth=max(default_font_size // 15 * output.scale, 1),\n",
    "    )\n",
    "    output.ax.add_patch(polygon)\n",
    "    return output\n",
    "\n",
    "def change_color_brightness(color, brightness_factor):\n",
    "    \"\"\"\n",
    "    Depending on the brightness_factor, gives a lighter or darker color i.e. a color with\n",
    "    less or more saturation than the original color.\n",
    "\n",
    "    Args:\n",
    "        color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n",
    "            formats that are accepted.\n",
    "        brightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n",
    "            0 will correspond to no change, a factor in [-1.0, 0) range will result in\n",
    "            a darker color and a factor in (0, 1.0] range will result in a lighter color.\n",
    "\n",
    "    Returns:\n",
    "        modified_color (tuple[double]): a tuple containing the RGB values of the\n",
    "            modified color. Each value in the tuple is in the [0.0, 1.0] range.\n",
    "    \"\"\"\n",
    "    assert brightness_factor >= -1.0 and brightness_factor <= 1.0\n",
    "    color = mplc.to_rgb(color)\n",
    "    polygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))\n",
    "    modified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])\n",
    "    modified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness\n",
    "    modified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness\n",
    "    modified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])\n",
    "    return tuple(np.clip(modified_color, 0.0, 1.0))\n",
    "\n",
    "def draw_text(\n",
    "    output,\n",
    "    text,\n",
    "    position,\n",
    "    *,\n",
    "    font_size=None,\n",
    "    color=\"g\",\n",
    "    horizontal_alignment=\"center\",\n",
    "    rotation=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text (str): class label\n",
    "        position (tuple): a tuple of the x and y coordinates to place text on image.\n",
    "        font_size (int, optional): font of the text. If not provided, a font size\n",
    "            proportional to the image width is calculated and used.\n",
    "        color: color of the text. Refer to `matplotlib.colors` for full list\n",
    "            of formats that are accepted.\n",
    "        horizontal_alignment (str): see `matplotlib.text.Text`\n",
    "        rotation: rotation angle in degrees CCW\n",
    "\n",
    "    Returns:\n",
    "        output (VisImage): image object with text drawn.\n",
    "    \"\"\"\n",
    "    if not font_size:\n",
    "        font_size = default_font_size\n",
    "\n",
    "    # since the text background is dark, we don't want the text to be dark\n",
    "    color = np.maximum(list(mplc.to_rgb(color)), 0.2)\n",
    "    color[np.argmax(color)] = max(0.8, np.max(color))\n",
    "\n",
    "    x, y = position\n",
    "    output.ax.text(\n",
    "        x,\n",
    "        y,\n",
    "        text,\n",
    "        size=font_size * output.scale,\n",
    "        family=\"sans-serif\",\n",
    "        bbox={\"facecolor\": \"black\", \"alpha\": 0.8, \"pad\": 0.7, \"edgecolor\": \"none\"},\n",
    "        verticalalignment=\"top\",\n",
    "        horizontalalignment=horizontal_alignment,\n",
    "        color=color,\n",
    "        zorder=10,\n",
    "        rotation=rotation,\n",
    "    )\n",
    "    return output\n",
    "\n",
    "num_instances = 0\n",
    "if boxes is not None:\n",
    "    boxes = convert_boxes(boxes)\n",
    "    num_instances = len(boxes)\n",
    "\n",
    "if masks is not None:\n",
    "    masks = convert_masks(masks)\n",
    "    if num_instances:\n",
    "        assert len(masks) == num_instances\n",
    "    else:\n",
    "        num_instances = len(masks)\n",
    "\n",
    "if labels is not None:\n",
    "    assert len(labels) == num_instances\n",
    "assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = None\n",
    "if boxes is not None:\n",
    "    areas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\n",
    "elif masks is not None:\n",
    "    areas = np.asarray([x.area() for x in masks])\n",
    "\n",
    "if areas is not None:\n",
    "    sorted_idxs = np.argsort(-areas).tolist()\n",
    "    # Re-order overlapped instances in descending order.\n",
    "    boxes = boxes[sorted_idxs] if boxes is not None else None\n",
    "    labels = [labels[k] for k in sorted_idxs] if labels is not None else None\n",
    "    masks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\n",
    "    assigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\n",
    "\n",
    "for i in range(num_instances):\n",
    "    color = assigned_colors[i]\n",
    "    if boxes is not None:\n",
    "        output = draw_box(output, boxes[i], edge_color=color)\n",
    "\n",
    "    if masks is not None:\n",
    "        for segment in masks[i].polygons:\n",
    "            output = draw_polygon(output, segment.reshape(-1, 2), color, alpha=alpha)\n",
    "\n",
    "    if labels is not None:\n",
    "        # first get a box\n",
    "        if boxes is not None:\n",
    "            x0, y0, x1, y1 = boxes[i]\n",
    "            text_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\n",
    "            horiz_align = \"left\"\n",
    "        elif masks is not None:\n",
    "            # skip small mask without polygon\n",
    "            if len(masks[i].polygons) == 0:\n",
    "                continue\n",
    "\n",
    "            x0, y0, x1, y1 = masks[i].bbox()\n",
    "\n",
    "            # draw text in the center (defined by median) when box is not drawn\n",
    "            # median is less sensitive to outliers.\n",
    "            text_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\n",
    "            horiz_align = \"center\"\n",
    "        else:\n",
    "            continue  # drawing the box confidence for keypoints isn't very useful.\n",
    "        # for small objects, draw text at the side to avoid occlusion\n",
    "        instance_area = (y1 - y0) * (x1 - x0)\n",
    "        if (\n",
    "            instance_area < _SMALL_OBJECT_AREA_THRESH * output.scale\n",
    "            or y1 - y0 < 40 * output.scale\n",
    "        ):\n",
    "            if y1 >= output.height - 5:\n",
    "                text_pos = (x1, y0)\n",
    "            else:\n",
    "                text_pos = (x0, y1)\n",
    "\n",
    "        height_ratio = (y1 - y0) / np.sqrt(output.height * output.width)\n",
    "        lighter_color = change_color_brightness(color, brightness_factor=0.7)\n",
    "        font_size = (\n",
    "            np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n",
    "            * 0.5\n",
    "            * default_font_size\n",
    "        )\n",
    "        vis_output = draw_text(\n",
    "            output,\n",
    "            labels[i],\n",
    "            text_pos,\n",
    "            color=lighter_color,\n",
    "            horizontal_alignment=horiz_align,\n",
    "            font_size=font_size,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_MaskDino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
