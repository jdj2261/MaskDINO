{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask-Dino Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# fmt: off\n",
    "import sys\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.insert(1, home_dir)\n",
    "print(home_dir)\n",
    "os.environ[\"DETECTRON2_DATASETS\"] = \"../datasets\"\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as mplc\n",
    "import matplotlib.figure as mplfigure\n",
    "import colorsys\n",
    "from pprint import pprint\n",
    "\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data.detection_utils import read_image\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.data import detection_utils as utils\n",
    "\n",
    "from detectron2.utils.visualizer import VisImage, _create_text_labels, GenericMask\n",
    "from detectron2.structures import ImageList, BitMasks, Boxes, BoxMode, Keypoints, PolygonMasks, RotatedBoxes\n",
    "from detectron2.utils.colormap import random_color\n",
    "\n",
    "from detectron2.data import (\n",
    "    MetadataCatalog,\n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    ")\n",
    "\n",
    "from maskdino.utils import box_ops\n",
    "from maskdino import add_maskdino_config\n",
    "from maskdino import COCOInstanceNewBaselineDatasetMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cfg(args):\n",
    "    # load config from file and command-line arguments\n",
    "    cfg = get_cfg()\n",
    "    add_deeplab_config(cfg)\n",
    "    add_maskdino_config(cfg)\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"maskdino demo for builtin configs\")\n",
    "    parser.add_argument(\n",
    "        \"--config-file\",\n",
    "        default=\"../configs/coco/instance-segmentation/maskdino_R50_bs16_50ep_3s.yaml\",\n",
    "        metavar=\"FILE\",\n",
    "        help=\"path to config file\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input\",\n",
    "        nargs=\"+\",\n",
    "        help=\"A list of space separated input images; \"\n",
    "        \"or a single glob pattern such as 'directory/*.jpg'\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--opts\",\n",
    "        help=\"Modify config options using the command-line 'KEY VALUE' pairs\",\n",
    "        default=[],\n",
    "        nargs=argparse.REMAINDER,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        help=\"A file or directory to save output visualizations. \"\n",
    "        \"If not given, will show output in an OpenCV window.\",\n",
    "    )\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method(\"spawn\", force=True)\n",
    "args = get_parser().parse_args('')\n",
    "\n",
    "args.input = [\"../images/fruit.jpg\"]\n",
    "args.opts = ['MODEL.WEIGHTS', '../ckpts/maskdino_r50_50ep_300q_hid1024_3sd1_instance_maskenhanced_mask46.1ap_box51.5ap.pth']\n",
    "args.output = home_dir + \"/outputs\"\n",
    "cfg = setup_cfg(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = COCOInstanceNewBaselineDatasetMapper(cfg, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = build_detection_train_loader(cfg, mapper=mapper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(cfg)\n",
    "model.train()\n",
    "\n",
    "if len(cfg.DATASETS.TEST):\n",
    "    metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "\n",
    "checkpointer = DetectionCheckpointer(model)\n",
    "checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "aug = T.ResizeShortestEdge([cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST)\n",
    "input_format = cfg.INPUT.FORMAT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone (ResNet)\n",
    "- input shape : [B, 3, H, W]\n",
    "- output shape\n",
    "    - level 1 shape : [B, 256, H/4, W/4]\n",
    "    - level 2 shape : [B, 512, H/8, W/8]\n",
    "    - level 3 shape : [B, 1024, H/16, W/16]\n",
    "    - level 4 shape : [B, 2048, H/32, W/32]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_targets(targets, images):\n",
    "    h_pad, w_pad = images.tensor.shape[-2:]\n",
    "    new_targets = []\n",
    "    for targets_per_image in targets:\n",
    "        # pad gt\n",
    "        h, w = targets_per_image.image_size\n",
    "        image_size_xyxy = torch.as_tensor([w, h, w, h], dtype=torch.float, device=model.device)\n",
    "\n",
    "        gt_masks = targets_per_image.gt_masks\n",
    "        padded_masks = torch.zeros((gt_masks.shape[0], h_pad, w_pad), dtype=gt_masks.dtype, device=gt_masks.device)\n",
    "        padded_masks[:, : gt_masks.shape[1], : gt_masks.shape[2]] = gt_masks\n",
    "        new_targets.append(\n",
    "            {\n",
    "                \"labels\": targets_per_image.gt_classes,\n",
    "                \"masks\": padded_masks,\n",
    "                \"boxes\":box_ops.box_xyxy_to_cxcywh(targets_per_image.gt_boxes.tensor)/image_size_xyxy\n",
    "            }\n",
    "        )\n",
    "    return new_targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batched_inputs in data_loader:\n",
    "#     loss_dict = model(batched_inputs)\n",
    "#     print(loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_iter_obj = iter(data_loader)\n",
    "data = next(data_loader_iter_obj)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    images = [x[\"image\"].to(model.device) for x in data]\n",
    "    images = [(x - model.pixel_mean) / model.pixel_std for x in images]\n",
    "    images = ImageList.from_tensors(images, model.size_divisibility)\n",
    "    \n",
    "    print(\"|Input Shape|\")\n",
    "    print(f\"  {images.tensor.shape}\")\n",
    "    features = model.backbone(images.tensor)\n",
    "\n",
    "    print(\"|Backbone Output|\")\n",
    "    for lvl, f in enumerate(features):\n",
    "        print(f\"  level:{lvl}, {features[f].shape}\")\n",
    "\n",
    "    gt_instances = [x[\"instances\"].to(model.device) for x in data]\n",
    "    targets = prepare_targets(gt_instances, images)\n",
    "\n",
    "    print(\"|Pixel Decoder Output|\")\n",
    "    mask_features, transformer_encoder_features, multi_scale_features = model.sem_seg_head.pixel_decoder.forward_features(features, None)\n",
    "    print(f\"  mask_features: {mask_features.shape}\")\n",
    "    print(f\"  transformer_encoder_features: {transformer_encoder_features.shape}\")\n",
    "    print(f\"  multi_scale_features\")\n",
    "    for lvl, f in enumerate(multi_scale_features):\n",
    "        print(f\"    level:{lvl}, {f.shape}\")\n",
    "\n",
    "    print(\"|Transformer Decoder Output|\")\n",
    "    outputs, mask_dict = model.sem_seg_head.predictor(multi_scale_features, mask_features, None, targets=targets)\n",
    "    print(outputs.keys(), mask_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = read_image(args.input[0], format=\"BGR\")\n",
    "with torch.no_grad():\n",
    "    if input_format == \"RGB\":\n",
    "        # whether the model expects BGR inputs or RGB\n",
    "        original_image = img[:, :, ::-1]\n",
    "    height, width = original_image.shape[:2]\n",
    "    image = aug.get_transform(original_image).apply_image(original_image)\n",
    "    image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "    image.to(cfg.MODEL.DEVICE)\n",
    "\n",
    "    batched_inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "    batched_inputs = [batched_inputs]\n",
    "    images = [x[\"image\"].to(model.device) for x in batched_inputs]\n",
    "    images = [(x - model.pixel_mean) / model.pixel_std for x in images]\n",
    "    images = ImageList.from_tensors(images, model.size_divisibility)\n",
    "    \n",
    "    print(\"|Input Shape|\")\n",
    "    print(f\"  {images.tensor.shape}\")\n",
    "    features = model.backbone(images.tensor)\n",
    "\n",
    "    print(\"|Backbone Output|\")\n",
    "    for lvl, f in enumerate(features):\n",
    "        print(f\"  level:{lvl}, {features[f].shape}\")\n",
    "\n",
    "    print(\"|Pixel Decoder Output|\")\n",
    "    mask_features, transformer_encoder_features, multi_scale_features = model.sem_seg_head.pixel_decoder.forward_features(features, None)\n",
    "    print(f\"  mask_features: {mask_features.shape}\")\n",
    "    print(f\"  transformer_encoder_features: {transformer_encoder_features.shape}\")\n",
    "    print(f\"  multi_scale_features\")\n",
    "    for lvl, f in enumerate(multi_scale_features):\n",
    "        print(f\"    level:{lvl}, {f.shape}\")\n",
    "\n",
    "    print(\"|Transformer Decoder Output|\")\n",
    "    predictions = model.sem_seg_head.predictor(multi_scale_features, mask_features, None, targets=targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for path in tqdm.tqdm(args.input):\n",
    "    img = read_image(path, format=\"BGR\")\n",
    "\n",
    "    print(img.shape)\n",
    "    with torch.no_grad():\n",
    "        if input_format == \"RGB\":\n",
    "            # whether the model expects BGR inputs or RGB\n",
    "            original_image = img[:, :, ::-1]\n",
    "        height, width = original_image.shape[:2]\n",
    "        image = aug.get_transform(original_image).apply_image(original_image)\n",
    "        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "        image.to(cfg.MODEL.DEVICE)\n",
    "\n",
    "        inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "\n",
    "        print(image.shape, height, width)\n",
    "        predictions = model([inputs])[0]\n",
    "\n",
    "        out_filename = os.path.join(args.output, os.path.basename(path))\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img[:, :, ::-1]\n",
    "img = np.asarray(img).clip(0, 255).astype(np.uint8)\n",
    "output = VisImage(img, scale=1.0)\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "instances = predictions[\"instances\"].to(cpu_device)\n",
    "\n",
    "boxes = instances.pred_boxes if instances.has(\"pred_boxes\") else None\n",
    "scores = instances.scores if instances.has(\"scores\") else None\n",
    "classes = instances.pred_classes.tolist() if instances.has(\"pred_classes\") else None\n",
    "labels = _create_text_labels(classes, scores, metadata.get(\"thing_classes\", None))\n",
    "\n",
    "masks = np.asarray(instances.pred_masks)\n",
    "masks = [GenericMask(x, output.height, output.width) for x in masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = None\n",
    "alpha = 0.5\n",
    "default_font_size = max(\n",
    "    np.sqrt(output.height * output.width) // 90, 10 // 1.0\n",
    ")\n",
    "_SMALL_OBJECT_AREA_THRESH = 1000\n",
    "\n",
    "def convert_boxes(boxes):\n",
    "    \"\"\"\n",
    "    Convert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n",
    "    \"\"\"\n",
    "    if isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\n",
    "        return boxes.tensor.detach().numpy()\n",
    "    else:\n",
    "        return np.asarray(boxes)\n",
    "\n",
    "def convert_masks(masks_or_polygons):\n",
    "    \"\"\"\n",
    "    Convert different format of masks or polygons to a tuple of masks and polygons.\n",
    "\n",
    "    Returns:\n",
    "        list[GenericMask]:\n",
    "    \"\"\"\n",
    "\n",
    "    m = masks_or_polygons\n",
    "    if isinstance(m, PolygonMasks):\n",
    "        m = m.polygons\n",
    "    if isinstance(m, BitMasks):\n",
    "        m = m.tensor.numpy()\n",
    "    if isinstance(m, torch.Tensor):\n",
    "        m = m.numpy()\n",
    "    ret = []\n",
    "    for x in m:\n",
    "        if isinstance(x, GenericMask):\n",
    "            ret.append(x)\n",
    "        else:\n",
    "            ret.append(GenericMask(x, output.height, output.width))\n",
    "    return ret\n",
    "\n",
    "def draw_box(output, box_coord, alpha=0.5, edge_color=\"g\", line_style=\"-\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        box_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\n",
    "            are the coordinates of the image's top left corner. x1 and y1 are the\n",
    "            coordinates of the image's bottom right corner.\n",
    "        alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
    "        edge_color: color of the outline of the box. Refer to `matplotlib.colors`\n",
    "            for full list of formats that are accepted.\n",
    "        line_style (string): the string to use to create the outline of the boxes.\n",
    "\n",
    "    Returns:\n",
    "        output (VisImage): image object with box drawn.\n",
    "    \"\"\"\n",
    "    x0, y0, x1, y1 = box_coord\n",
    "    width = x1 - x0\n",
    "    height = y1 - y0\n",
    "\n",
    "    linewidth = max(default_font_size / 4, 1)\n",
    "\n",
    "    output.ax.add_patch(\n",
    "        mpl.patches.Rectangle(\n",
    "            (x0, y0),\n",
    "            width,\n",
    "            height,\n",
    "            fill=False,\n",
    "            edgecolor=edge_color,\n",
    "            linewidth=linewidth * output.scale,\n",
    "            alpha=alpha,\n",
    "            linestyle=line_style,\n",
    "        )\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def draw_polygon(output, segment, color, edge_color=None, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        segment: numpy array of shape Nx2, containing all the points in the polygon.\n",
    "        color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n",
    "            formats that are accepted.\n",
    "        edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n",
    "            full list of formats that are accepted. If not provided, a darker shade\n",
    "            of the polygon color will be used instead.\n",
    "        alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
    "\n",
    "    Returns:\n",
    "        output (VisImage): image object with polygon drawn.\n",
    "    \"\"\"\n",
    "    if edge_color is None:\n",
    "        # make edge color darker than the polygon color\n",
    "        if alpha > 0.8:\n",
    "            edge_color = change_color_brightness(color, brightness_factor=-0.7)\n",
    "        else:\n",
    "            edge_color = color\n",
    "    edge_color = mplc.to_rgb(edge_color) + (1,)\n",
    "\n",
    "    polygon = mpl.patches.Polygon(\n",
    "        segment,\n",
    "        fill=True,\n",
    "        facecolor=mplc.to_rgb(color) + (alpha,),\n",
    "        edgecolor=edge_color,\n",
    "        linewidth=max(default_font_size // 15 * output.scale, 1),\n",
    "    )\n",
    "    output.ax.add_patch(polygon)\n",
    "    return output\n",
    "\n",
    "def change_color_brightness(color, brightness_factor):\n",
    "    \"\"\"\n",
    "    Depending on the brightness_factor, gives a lighter or darker color i.e. a color with\n",
    "    less or more saturation than the original color.\n",
    "\n",
    "    Args:\n",
    "        color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n",
    "            formats that are accepted.\n",
    "        brightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n",
    "            0 will correspond to no change, a factor in [-1.0, 0) range will result in\n",
    "            a darker color and a factor in (0, 1.0] range will result in a lighter color.\n",
    "\n",
    "    Returns:\n",
    "        modified_color (tuple[double]): a tuple containing the RGB values of the\n",
    "            modified color. Each value in the tuple is in the [0.0, 1.0] range.\n",
    "    \"\"\"\n",
    "    assert brightness_factor >= -1.0 and brightness_factor <= 1.0\n",
    "    color = mplc.to_rgb(color)\n",
    "    polygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))\n",
    "    modified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])\n",
    "    modified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness\n",
    "    modified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness\n",
    "    modified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])\n",
    "    return tuple(np.clip(modified_color, 0.0, 1.0))\n",
    "\n",
    "def draw_text(\n",
    "    output,\n",
    "    text,\n",
    "    position,\n",
    "    *,\n",
    "    font_size=None,\n",
    "    color=\"g\",\n",
    "    horizontal_alignment=\"center\",\n",
    "    rotation=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text (str): class label\n",
    "        position (tuple): a tuple of the x and y coordinates to place text on image.\n",
    "        font_size (int, optional): font of the text. If not provided, a font size\n",
    "            proportional to the image width is calculated and used.\n",
    "        color: color of the text. Refer to `matplotlib.colors` for full list\n",
    "            of formats that are accepted.\n",
    "        horizontal_alignment (str): see `matplotlib.text.Text`\n",
    "        rotation: rotation angle in degrees CCW\n",
    "\n",
    "    Returns:\n",
    "        output (VisImage): image object with text drawn.\n",
    "    \"\"\"\n",
    "    if not font_size:\n",
    "        font_size = default_font_size\n",
    "\n",
    "    # since the text background is dark, we don't want the text to be dark\n",
    "    color = np.maximum(list(mplc.to_rgb(color)), 0.2)\n",
    "    color[np.argmax(color)] = max(0.8, np.max(color))\n",
    "\n",
    "    x, y = position\n",
    "    output.ax.text(\n",
    "        x,\n",
    "        y,\n",
    "        text,\n",
    "        size=font_size * output.scale,\n",
    "        family=\"sans-serif\",\n",
    "        bbox={\"facecolor\": \"black\", \"alpha\": 0.8, \"pad\": 0.7, \"edgecolor\": \"none\"},\n",
    "        verticalalignment=\"top\",\n",
    "        horizontalalignment=horizontal_alignment,\n",
    "        color=color,\n",
    "        zorder=10,\n",
    "        rotation=rotation,\n",
    "    )\n",
    "    return output\n",
    "\n",
    "num_instances = 0\n",
    "if boxes is not None:\n",
    "    boxes = convert_boxes(boxes)\n",
    "    num_instances = len(boxes)\n",
    "\n",
    "if masks is not None:\n",
    "    masks = convert_masks(masks)\n",
    "    if num_instances:\n",
    "        assert len(masks) == num_instances\n",
    "    else:\n",
    "        num_instances = len(masks)\n",
    "\n",
    "if labels is not None:\n",
    "    assert len(labels) == num_instances\n",
    "assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = None\n",
    "if boxes is not None:\n",
    "    areas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\n",
    "elif masks is not None:\n",
    "    areas = np.asarray([x.area() for x in masks])\n",
    "\n",
    "if areas is not None:\n",
    "    sorted_idxs = np.argsort(-areas).tolist()\n",
    "    # Re-order overlapped instances in descending order.\n",
    "    boxes = boxes[sorted_idxs] if boxes is not None else None\n",
    "    labels = [labels[k] for k in sorted_idxs] if labels is not None else None\n",
    "    masks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\n",
    "    assigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\n",
    "\n",
    "for i in range(num_instances):\n",
    "    color = assigned_colors[i]\n",
    "    if boxes is not None:\n",
    "        output = draw_box(output, boxes[i], edge_color=color)\n",
    "\n",
    "    if masks is not None:\n",
    "        for segment in masks[i].polygons:\n",
    "            output = draw_polygon(output, segment.reshape(-1, 2), color, alpha=alpha)\n",
    "\n",
    "    if labels is not None:\n",
    "        # first get a box\n",
    "        if boxes is not None:\n",
    "            x0, y0, x1, y1 = boxes[i]\n",
    "            text_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\n",
    "            horiz_align = \"left\"\n",
    "        elif masks is not None:\n",
    "            # skip small mask without polygon\n",
    "            if len(masks[i].polygons) == 0:\n",
    "                continue\n",
    "\n",
    "            x0, y0, x1, y1 = masks[i].bbox()\n",
    "\n",
    "            # draw text in the center (defined by median) when box is not drawn\n",
    "            # median is less sensitive to outliers.\n",
    "            text_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\n",
    "            horiz_align = \"center\"\n",
    "        else:\n",
    "            continue  # drawing the box confidence for keypoints isn't very useful.\n",
    "        # for small objects, draw text at the side to avoid occlusion\n",
    "        instance_area = (y1 - y0) * (x1 - x0)\n",
    "        if (\n",
    "            instance_area < _SMALL_OBJECT_AREA_THRESH * output.scale\n",
    "            or y1 - y0 < 40 * output.scale\n",
    "        ):\n",
    "            if y1 >= output.height - 5:\n",
    "                text_pos = (x1, y0)\n",
    "            else:\n",
    "                text_pos = (x0, y1)\n",
    "\n",
    "        height_ratio = (y1 - y0) / np.sqrt(output.height * output.width)\n",
    "        lighter_color = change_color_brightness(color, brightness_factor=0.7)\n",
    "        font_size = (\n",
    "            np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n",
    "            * 0.5\n",
    "            * default_font_size\n",
    "        )\n",
    "        vis_output = draw_text(\n",
    "            output,\n",
    "            labels[i],\n",
    "            text_pos,\n",
    "            color=lighter_color,\n",
    "            horizontal_alignment=horiz_align,\n",
    "            font_size=font_size,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_MaskDino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
